{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6931f3e1-1a60-4e76-b5d6-47dbff9bf74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torchvision \n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0124d051-fc48-4cd5-bba9-e3c3f1d50e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), \n",
    "     transforms.Resize((227, 227)),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "\n",
    "# already downloaded\n",
    "train = torchvision.datasets.CIFAR10(root='./data/train_CIFAR10', train=True,\n",
    "                                    download=False, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=64,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "# already downloaded \n",
    "test = torchvision.datasets.CIFAR10(root='./data/test_CIFAR10', train=False,\n",
    "                                    download=False, transform=transform)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(train, batch_size=64,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "62c6bcdb-5e4f-44ed-a5a8-9381c7366b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(nn.Module):\n",
    "    def __init__(self, in_channels, out_1, red_3, out_3, red_5, out_5, out_pool):\n",
    "        super(Inception, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_1, kernel_size=1) \n",
    "        \n",
    "        self.conv3 = nn.Sequential( \n",
    "            nn.Conv2d(in_channels, red_3, kernel_size=1, padding=0),\n",
    "            nn.Conv2d(red_3, out_3, kernel_size=3, padding=1))\n",
    "        \n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, red_5, kernel_size=1), \n",
    "            nn.Conv2d(red_5, out_5, kernel_size=5, padding=2)) \n",
    "        \n",
    "        self.max = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, padding=1, stride=1), \n",
    "            nn.Conv2d(in_channels, out_pool, kernel_size=1))\n",
    "        \n",
    "    def forward(self, x): \n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv3(x)\n",
    "        x3 = self.conv5(x)\n",
    "        x4 = self.max(x)\n",
    "        out = torch.cat([x1, x2, x3, x4], dim=1)\n",
    "        return out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class GooglLeNet(nn.Module): \n",
    "    def __init__(self, out_channels=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3), \n",
    "            nn.MaxPool2d(kernel_size=3, stride=2))\n",
    "             \n",
    "        self.l2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 192, kernel_size=3, stride=1, padding=1), \n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "        \n",
    "        \n",
    "        self.inc3a = Inception(192, 64, 96, 128, 16, 32, 32)\n",
    "        self.inc3b = Inception(256, 128, 128, 192, 32, 96, 64)\n",
    "        self.max3 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "            \n",
    "        self.l4 = nn.Sequential(\n",
    "            Inception(480, 192, 96, 208, 16, 48, 64), \n",
    "            Inception(512, 160, 112, 224, 24, 64, 64), \n",
    "            Inception(512, 128, 128, 256, 24, 64, 64), \n",
    "            Inception(512, 112, 144, 288, 32, 64, 64), \n",
    "            Inception(528, 256, 160, 320, 32, 128, 128),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)) \n",
    "    \n",
    "        \n",
    "        self.inc5a = Inception(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.inc5b = Inception(832, 384, 192, 384, 48, 128, 128) \n",
    "        self.avg = nn.AvgPool2d(kernel_size=7, stride=1)\n",
    "        \n",
    "        self.drop = nn.Dropout(p=0.4)\n",
    "            \n",
    "        self.linear = nn.Linear(1024,out_channels)\n",
    "    \n",
    "    def forward(self, x): \n",
    "        x = self.l1(x) \n",
    "        x = self.l2(x)\n",
    "        x = self.inc3a(x) \n",
    "        x = self.inc3b(x) \n",
    "        x = self.max3(x) \n",
    "        x = self.l4(x)\n",
    "        x = self.inc5a(x) \n",
    "        print(x.shape)\n",
    "        x = self.inc5b(x) \n",
    "        x = self.avg(x) \n",
    "        x = self.drop(x) \n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.linear(x) \n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3df6a1b4-a569-4223-93a5-2083adf67aeb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class NaiveInception(nn.Module):\n",
    "#     '''\n",
    "#     Computer can likely only tolerate Naive inception model\n",
    "#     '''\n",
    "#     def __init__(self, in_channels, out_1, out_3, out_5):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.conv1 = nn.Conv2d(in_channels, out_1, kernel_size=1, stride=1, padding=0)\n",
    "#         self.conv2 = nn.Conv2d(in_channels, out_3, kernel_size=3, stride=1, padding=1)\n",
    "#         self.conv3 = nn.Conv2d(in_channels, out_5, kernel_size=5, stride=1, padding=2)            \n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1) \n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x1 = self.conv1(x)\n",
    "#         x2 = self.conv2(x)\n",
    "#         x3 = self.conv3(x)\n",
    "#         x4 = self.maxpool(x)\n",
    "#         out = torch.cat([x1, x2, x3, x4], dim=1)\n",
    "#         return out\n",
    "    \n",
    "    \n",
    "    \n",
    "# class GooglLeNetNaive(nn.Module): \n",
    "#     def __init__(self, out_channels):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.l1 = nn.Sequential(\n",
    "#             nn.Conv2d(3, 64, kernel_size=7, stride=2), \n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2))\n",
    "             \n",
    "#         self.l2 = nn.Sequential(\n",
    "#             nn.Conv2d(64, 192, kernel_size=3, stride=1), \n",
    "#             nn.MaxPool2d(kernel_size=3, stride=1))\n",
    "        \n",
    "#         self.l3 = nn.Sequential(\n",
    "#             NaiveInception(192, 256),\n",
    "#             NaiveInception(256, 480), \n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2))\n",
    "            \n",
    "#         self.l4 = nn.Sequential(\n",
    "#             NaiveInception(512, 512), \n",
    "#             NaiveInception(512, 512), \n",
    "#             NaiveInception(512, 528), \n",
    "#             NaiveInception(528, 832), \n",
    "#             nn.MaxPool2d(kernel_size=3, stride=2)) \n",
    "    \n",
    "#         self.l5 = nn.Sequential(\n",
    "#             NaiveInception(832, 1024), \n",
    "#             NaiveInception(1024, 1024), \n",
    "#             nn.AvgPool2d(kernel_size=7, stride=1), \n",
    "#             nn.Dropout(p=0.4))\n",
    "            \n",
    "#         self.linear = nn.Linear(1000,1000)\n",
    "    \n",
    "#     def forward(self, x): \n",
    "#         out = self.l1(x) \n",
    "#         out = self.l2(out)\n",
    "#         out = self.l3(out) \n",
    "#         out = self.l4(out) \n",
    "#         out = self.l5(out) \n",
    "#         out = self.linear(out) \n",
    "#         out = nn.Softmax()\n",
    "        \n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "07b00d9c-acbe-4c2b-a6e0-ff645f068bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop \n",
    "torch.manual_seed(423984) \n",
    "googlenet = GooglLeNet()\n",
    "googlenet.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer parameters according to paper\n",
    "optimizer = torch.optim.SGD(googlenet.parameters(),lr=0.01, momentum=0.9, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f7175908-753d-441d-befb-0a671f401ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55a8ddf9d3694a5bade5ba57992fbc0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/finhardy/opt/anaconda3/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/finhardy/opt/anaconda3/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIxEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
      "  Referenced from: <F30FD7F2-B214-3D4A-93DD-0D484FBE6931> /Users/finhardy/opt/anaconda3/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Expected in:     <9D4C7FD2-49A8-383A-AC3E-A560DE81B0D8> /Users/finhardy/opt/anaconda3/lib/python3.9/site-packages/torch/lib/libtorch_cpu.dylib\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/Users/finhardy/opt/anaconda3/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/finhardy/opt/anaconda3/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIxEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
      "  Referenced from: <F30FD7F2-B214-3D4A-93DD-0D484FBE6931> /Users/finhardy/opt/anaconda3/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Expected in:     <9D4C7FD2-49A8-383A-AC3E-A560DE81B0D8> /Users/finhardy/opt/anaconda3/lib/python3.9/site-packages/torch/lib/libtorch_cpu.dylib\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size: torch.Size([64, 3, 227, 227])\n",
      "Output from Conv: torch.Size([64, 64, 56, 56])\n",
      "Output from Conv: torch.Size([64, 192, 28, 28])\n",
      "Shape after inception 3a: torch.Size([64, 256, 28, 28])\n",
      "Shape after inception 3b: torch.Size([64, 480, 28, 28])\n",
      "Output from inception 3a + 3b: torch.Size([64, 480, 14, 14])\n",
      "Output from inception 4a-4d: torch.Size([64, 832, 7, 7])\n",
      "torch.Size([64, 832, 7, 7])\n",
      "input size: torch.Size([64, 3, 227, 227])\n",
      "Output from Conv: torch.Size([64, 64, 56, 56])\n",
      "Output from Conv: torch.Size([64, 192, 28, 28])\n",
      "Shape after inception 3a: torch.Size([64, 256, 28, 28])\n",
      "Shape after inception 3b: torch.Size([64, 480, 28, 28])\n",
      "Output from inception 3a + 3b: torch.Size([64, 480, 14, 14])\n",
      "Output from inception 4a-4d: torch.Size([64, 832, 7, 7])\n",
      "torch.Size([64, 832, 7, 7])\n",
      "input size: torch.Size([64, 3, 227, 227])\n",
      "Output from Conv: torch.Size([64, 64, 56, 56])\n",
      "Output from Conv: torch.Size([64, 192, 28, 28])\n",
      "Shape after inception 3a: torch.Size([64, 256, 28, 28])\n",
      "Shape after inception 3b: torch.Size([64, 480, 28, 28])\n",
      "Output from inception 3a + 3b: torch.Size([64, 480, 14, 14])\n",
      "Output from inception 4a-4d: torch.Size([64, 832, 7, 7])\n",
      "torch.Size([64, 832, 7, 7])\n",
      "input size: torch.Size([64, 3, 227, 227])\n",
      "Output from Conv: torch.Size([64, 64, 56, 56])\n",
      "Output from Conv: torch.Size([64, 192, 28, 28])\n",
      "Shape after inception 3a: torch.Size([64, 256, 28, 28])\n",
      "Shape after inception 3b: torch.Size([64, 480, 28, 28])\n",
      "Output from inception 3a + 3b: torch.Size([64, 480, 14, 14])\n",
      "Output from inception 4a-4d: torch.Size([64, 832, 7, 7])\n",
      "torch.Size([64, 832, 7, 7])\n",
      "input size: torch.Size([64, 3, 227, 227])\n",
      "Output from Conv: torch.Size([64, 64, 56, 56])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/s9/p9rn5jps1rqglt62bhljb0780000gn/T/ipykernel_2575/2932253410.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgooglenet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/s9/p9rn5jps1rqglt62bhljb0780000gn/T/ipykernel_2575/47480777.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Output from Conv: {x.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Output from Conv: {x.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minc3a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm \n",
    "\n",
    "torch.manual_seed(424)\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f'Epoch: {epoch}\\n-----')\n",
    "    \n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch, (x, y) in enumerate(train_loader): \n",
    "        x = x.to(device)\n",
    "        \n",
    "        y_pred = googlenet(x)\n",
    "        \n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss /= len(train)\n",
    "    \n",
    "    test_loss = 0\n",
    "    \n",
    "    with torch.inference_mode(): \n",
    "        for x, y in test:\n",
    "            test_pred = googlenet(x) \n",
    "            \n",
    "            loss = loss_fn(test_pred, y)\n",
    "            test_loss += loss\n",
    "            \n",
    "        test_loss /= len(test)\n",
    "    \n",
    "    print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
